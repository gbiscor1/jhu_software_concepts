# module_3/tests/test_controller_with_cli_mock.py
# File generated by GPT
from __future__ import annotations
import uuid
from pathlib import Path
from typing import Dict, Any

import json
import os

import pytest

from data.psych_connect import get_conn
from data.loader import load_applicants
from application.scrapper_cli_caller import Module2RunResult
from scripts.load_db import main as load_db_main


def _count_by_url(url: str) -> int:
    with get_conn() as conn, conn.cursor() as cur:
        cur.execute("SELECT COUNT(*) FROM applicants WHERE url = %s;", (url,))
        row = cur.fetchone()
        return int(row[0]) if row else 0


def _cleanup(urls: list[str]) -> None:
    if not urls:
        return
    with get_conn() as conn, conn.cursor() as cur:
        for u in urls:
            cur.execute("DELETE FROM applicants WHERE url = %s;", (u,))


def _row(url: str, **overrides: Dict[str, Any]) -> Dict[str, Any]:
    base = {
        "program": "Computer Science MS",
        "comments": "pytest controller",
        "date_added": "2025-09-01",
        "url": url,
        "status": "Accepted",
        "term": "Fall 2025",
        "citizenship": "American",
        "gpa": 3.7,
        "gre_total": 322,
        "gre_verbal": 159,
        "gre_aw": 4.5,
        "degree": "MS",
        "program_canon": "Computer Science",
        "university_canon": "Test University",
    }
    base.update(overrides)
    return base


def test_controller_calls_cli_and_upserts(monkeypatch, tmp_path, capsys):
    # Arrange unique URLs
    url_existing = f"https://example.com/pytest-existing-{uuid.uuid4()}"
    url_new = f"https://example.com/pytest-new-{uuid.uuid4()}"
    created = [url_existing, url_new]

    # Seed the existing row through the real loader to simulate prior data
    first = load_applicants([_row(url_existing)])
    assert first["attempted"] == 1 and first["inserted"] == 1

    # Create fake CLI output artefacts so copy step in controller succeeds
    raw = tmp_path / "applicant_data.json"
    cleaned = tmp_path / "applicant_data.cleaned.json"
    extended = tmp_path / "llm_extend_applicant_data.json"
    for p in (raw, cleaned, extended):
        p.write_text("[]", encoding="utf-8")

    # Build a fake Module2RunResult the controller will consume
    fake_run = Module2RunResult(
        pages=12,
        delay=0.8,
        use_llm=True,
        raw_path=raw,
        cleaned_path=cleaned,
        extended_path=extended,
        raw_count=0,
        cleaned_count=0,
        final_count=2,
        final_records=[_row(url_existing), _row(url_new)],  # dup + new
    )

    # Patch the CLI to return our fake result (no network/scrape)
    monkeypatch.setattr("scripts.load_db.run_module2_cli", lambda **kwargs: fake_run)


    # Act: run the real controller
    load_db_main()
    out = capsys.readouterr().out

    # Assert: exactly one new insert, one duplicate skip
    assert "'inserted': 1" in out
    assert "'skipped': 1" in out
    assert _count_by_url(url_new) >= 1

    # Cleanup DB rows we created
    _cleanup(created)

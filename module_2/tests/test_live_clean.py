"""TEST FILES GENERATED BY GPT"""


# tests/test_live_clean.py
import os
import re
import json
import pytest
from pathlib import Path

LIVE = os.getenv("LIVE_SCRAPE", "").strip() == "1"
pytestmark = pytest.mark.skipif(
    not LIVE, reason="Set LIVE_SCRAPE=1 to enable live web tests against thegradcafe.com"
)

BASE = "https://www.thegradcafe.com/survey/"

def test_live_scrape_then_clean_and_save(tmp_path):
    # Import here so local code is used
    from scrape import Scraper
    from clean import Cleaner, SCHEMA_FIELDS, STATUS_ALLOWED

    # --- scrape one page live ---
    s = Scraper(BASE, delay=0.0)
    url = s._build_page_url(1)
    html = s._fetch_page(url)
    assert html, "Expected non-empty HTML from live site"
    raw_rows = s._parse_page(html, url)
    assert isinstance(raw_rows, list) and len(raw_rows) > 0

    # --- clean ---
    cleaner = Cleaner(validate_with_dataclass=False)  # keep strict dataclass off for live smoke test
    cleaned = cleaner.clean_rows(raw_rows)
    assert isinstance(cleaned, list) and len(cleaned) > 0

    # --- shape / sanity checks on a few items ---
    assert set(cleaned[0].keys()) == set(SCHEMA_FIELDS)
    for r in cleaned[:10]:
        # SHALL present & non-empty
        for k in ("program", "university", "date_added", "url", "status"):
            assert isinstance(r[k], str) and r[k].strip(), f"{k} missing/empty"

        # canonical status
        assert r["status"] in STATUS_ALLOWED

        # ISO-ish date (YYYY-MM-DD)
        assert re.match(r"^\d{4}-\d{2}-\d{2}$", r["date_added"]) is not None

        # optional numeric types when present
        if r["gre_total"] is not None:
            assert isinstance(r["gre_total"], int)
        if r["gre_verbal"] is not None:
            assert isinstance(r["gre_verbal"], int)
        if r["gre_aw"] is not None:
            assert isinstance(r["gre_aw"], float)
        if r["gpa"] is not None:
            assert isinstance(r["gpa"], float)

    # no duplicate URLs after cleaning
    urls = [r["url"] for r in cleaned]
    assert len(urls) == len(set(urls)), "Expected deduped URLs"

    # --- save to tmp for the test ---
    out = tmp_path / "data" / "applicant_data.cleaned.json"
    out.parent.mkdir(parents=True, exist_ok=True)
    out.write_text(json.dumps(cleaned, ensure_ascii=False, indent=2), encoding="utf-8")
    assert out.exists()

    
    out_json = os.getenv("OUT_JSON", "").strip()
    if out_json:
        dst = Path(out_json)
        dst.parent.mkdir(parents=True, exist_ok=True)
        dst.write_text(out.read_text(encoding="utf-8"), encoding="utf-8")

"""TEST FILES GENERATED BY GPT"""

# tests/test_scrape_flow.py
import json
import pytest

def _row(page_url):
    # Minimal row matching our schema; values don't matter for flow tests
    return {
        "program": "P",
        "university": "U",
        "date_added": "2025-01-01",
        "url": page_url,
        "status": "Pending",
        "comments": None,
        "accept_date": None,
        "reject_date": None,
        "start_term": None,
        "start_year": None,
        "citizenship": None,
        "gre_total": None,
        "gre_verbal": None,
        "gre_aw": None,
        "degree": None,
        "gpa": None,
    }


def test_scrape_accumulates_and_saves(monkeypatch, tmp_path):
    from scrape import Scraper

    # No network: always “has HTML”
    def fake_fetch(self, url):
        return "<html>ok</html>"

    # No parsing: pretend each page has 2 rows
    def fake_parse(self, html, page_url):
        return [_row(page_url), _row(page_url)]

    monkeypatch.setattr(Scraper, "_fetch_page", fake_fetch)
    monkeypatch.setattr(Scraper, "_parse_page", fake_parse)

    s = Scraper("https://www.thegradcafe.com/survey/", delay=0.0)
    out_path = tmp_path / "data" / "applicant_data.json"
    rows = s.scrape(start_page=1, max_pages=3, out_path=str(out_path))

    # 3 pages × 2 rows
    assert len(rows) == 6
    assert out_path.exists()
    data = json.loads(out_path.read_text(encoding="utf-8"))
    assert isinstance(data, list) and len(data) == 6


def test_stops_on_empty_html(monkeypatch, tmp_path):
    from scrape import Scraper

    # Page 1 has HTML, page 2 is empty -> should stop after page 1
    def fake_fetch(self, url):
        return "<html>ok</html>" if "page=1" in url else ""

    def fake_parse(self, html, page_url):
        return [_row(page_url)]

    monkeypatch.setattr(Scraper, "_fetch_page", fake_fetch)
    monkeypatch.setattr(Scraper, "_parse_page", fake_parse)

    s = Scraper("https://www.thegradcafe.com/survey/", delay=0.0)
    out_path = tmp_path / "data" / "applicant_data.json"
    rows = s.scrape(start_page=1, max_pages=5, out_path=str(out_path))

    assert len(rows) == 1


def test_stops_on_no_rows(monkeypatch, tmp_path):
    from scrape import Scraper

    # Always returns HTML
    def fake_fetch(self, url):
        return "<html>ok</html>"

    # Rows on page 1, then empty on page 2 -> stop
    def fake_parse(self, html, page_url):
        return [_row(page_url)] if "page=1" in page_url else []

    monkeypatch.setattr(Scraper, "_fetch_page", fake_fetch)
    monkeypatch.setattr(Scraper, "_parse_page", fake_parse)

    s = Scraper("https://www.thegradcafe.com/survey/", delay=0.0)
    out_path = tmp_path / "data" / "applicant_data.json"
    rows = s.scrape(start_page=1, max_pages=10, out_path=str(out_path))

    assert len(rows) == 1


def test_build_page_url_with_existing_query():
    from scrape import Scraper
    s = Scraper("https://www.thegradcafe.com/survey/?sort=decision", delay=0.0)
    assert s._build_page_url(2).endswith("sort=decision&page=2")


def test_build_page_url_no_query():
    from scrape import Scraper
    s = Scraper("https://www.thegradcafe.com/survey/", delay=0.0)
    assert s._build_page_url(3).endswith("?page=3")
